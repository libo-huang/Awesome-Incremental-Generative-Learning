# Awesome Incremental / Continual / Lifelong Generative Learning
[![](https://awesome.re/badge.svg)](#awesome-incremental--continual--lifelong-generative-learning)
[![](https://img.shields.io/badge/Made%20with-Markdown-1f425f.svg)](#pushpin-outline)
[![](https://img.shields.io/badge/Issues-Open-1f425f.svg)](https://github.com/libo-huang/Awesome-Incremental-Generative-Learning/issues)
[![](https://img.shields.io/badge/Contributions-Welcome-1f425f)](#clap-contribute-chinese-version)
[![](https://img.shields.io/static/v1?label=%E2%AD%90&message=If%20Useful&style=flat&color=C7A5C0)](https://github.com/libo-huang/Awesome-Incremental-Generative-Learning)



## :pushpin: Outline
[:closed_book: Paper](#closed_book-paper)

&emsp; [2024](#2024) | [2023](#2023) | [2022](#2022) | [2021](#2021) | [2020](#2020) | [2019](#2019) | [2018](#2018) | [2017](#2017) | [Pre-2017](#pre-2017) 

[:clap: Contribute](#clap-contribute-chinese-version)

---






## :closed_book: Paper
### 2024
- (**AAAI 2024**) eTag: Class-Incremental Learning via Embedding Distillation and Task-Oriented Generation [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/29153)] [[code](https://github.com/libo-huang/eTag)] 
- (**CVPR 2024**) SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Kim_SDDGR_Stable_Diffusion-based_Deep_Generative_Replay_for_Class_Incremental_Object_CVPR_2024_paper.html)]
- (**CVPR 2024**) Generative Multi-modal Models are Good Class Incremental Learners [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_Generative_Multi-modal_Models_are_Good_Class_Incremental_Learners_CVPR_2024_paper.html)] [[code](https://github.com/DoubleClass/GMM)]
- (**CVPR 2024**) Online Task-Free Continual Generative and Discriminative Learning via Dynamic Cluster Memory [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_Online_Task-Free_Continual_Generative_and_Discriminative_Learning_via_Dynamic_Cluster_CVPR_2024_paper.html)] [[code](https://github.com/dtuzi123/DCM)]
- (**ICML 2024**) COPAL: Continual Pruning in Large Language Generative Models [[paper](https://openreview.net/forum?id=Lt8Lk7IQ5b)]
- (**ACMMM 2024**) Generating Prompts in Latent Space for Rehearsal-free Continual Learning [[paper](https://openreview.net/pdf?id=6HT4jUkSRg)] [[code](https://openreview.net/forum?id=6HT4jUkSRg)]
- (**TMLR 2024**) Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA [[paper](https://openreview.net/forum?id=TZdEgwZ6f3)]
- (**ICLR-tiny paper 2024**) KFC: Knowledge Reconstruction and Feedback Consolidation Enable Efficient and Effective Continual Generative Learning [[paper](https://openreview.net/pdf?id=pVTcR8ig3R)] [[code](https://github.com/libo-huang/KFC)]
- 	(**arXiv 2024**) Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond [[paper](https://arxiv.org/pdf/2408.02983)]
- 	(**arXiv 2024**) DiffClass: Diffusion-Based Class Incremental Learning [[paper](https://arxiv.org/pdf/2403.05016)]


### 2023
- (**ICCV 2023**) Lfs-gan: Lifelong few-shot image generation [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Seo_LFS-GAN_Lifelong_Few-Shot_Image_Generation_ICCV_2023_paper.html)] [[code](https://github.com/KHU-AGI/LFS-GAN)]
- (**ICCV 2023**) Generating Instance-level Prompts for Rehearsal-free Continual Learning [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Jung_Generating_Instance-level_Prompts_for_Rehearsal-free_Continual_Learning_ICCV_2023_paper.html)] [[code](https://github.com/naver-ai/dap-cl)]
- (**ICCV 2023**) When Prompt-based Incremental Learning Does Not Meet Strong Pretraining [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Tang_When_Prompt-based_Incremental_Learning_Does_Not_Meet_Strong_Pretraining_ICCV_2023_paper.html)] [[code](https://github.com/TOM-tym/APG)]
- (**ICCV 2023**) What does a platypus look like? Generating customized prompts for zero-shot image classification [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.html)] [[code](https://github.com/sarahpratt/CuPL)]
- (**ICML 2023**) Poisoning Generative Replay in Continual Learning to Promote Forgetting [[paper](https://proceedings.mlr.press/v202/kang23c.html)] [[code](https://www.dropbox.com/scl/fo/ae954h8tsjd6z138x7yf5/ACVvowDAq4C9cjJgUXuNJKw?rlkey=nhqo08bd7tzoxd0g6w2y5oijc&e=1&st=an4xuj5w&dl=0)]
- (**ICML 2023**) DDGR: Continual Learning with Deep Diffusion-based Generative Replay [[paper](https://proceedings.mlr.press/v202/gao23e)] [[code](https://github.com/xiaocangshengGR/DDGR)]
- (**NeurIPS 2023**) Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/376276a95781fa17c177b1ccdd0a03ac-Abstract-Conference.html)] [[code](https://github.com/clear-nus/selective-amnesia)]
- (**ICLR 2023**) Better Generative Replay for Continual Federated Learning [[paper](https://openreview.net/forum?id=cRxYWKiTan)] [[code](https://github.com/daiqing98/FedCIL)]
- (**Neural Network 2023**) Generative negative replay for continual learning [[paper](https://www.sciencedirect.com/science/article/pii/S0893608023001235)] [[code](https://openreview.net/forum?id=MWQCPYSJRN)]


### 2022
- (**ECCV 2022**) Generative Negative Text Replay for Continual Vision-Language Pretraining [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960022.pdf)]
- (**ACL 2022**) Continual Sequence Generation with Adaptive Compositional Modules [[paper](https://aclanthology.org/2022.acl-long.255/)] [[code](https://github.com/SALT-NLP/Adaptive-Compositional-Modules)]
- (**Journal of Image 2022**) Unified probabilistic deep continual learning through generative replay and open set recognition [[paper](https://d1wqtxts1xzle7.cloudfront.net/92524318/pdf-libre.pdf?1665928933=&response-content-disposition=inline%3B+filename%3DUnified_Probabilistic_Deep_Continual_Lea.pdf&Expires=1722779706&Signature=LnvZgOp795QVK-4SzuUAwZLwdvIROMY~Mbzb3Q8e8cHOIwFitPMdh7wlO3fk2xY-tpu60g-KT3U3F-9oWy-X52xJ0~Dwrvet-pCZkoJffvwlfPO1rjsT1y~tpRj7O7CnU-hycrdmYo3rhg~IKHYIwUYEgYOvi1wTsj2Zl0iVMbGfJwigu3OMh0WvEgsXzHTAf9PUj~wqk8zYrUfrxjrY~SfUcqV2Z7SfAwGII8Fmixa2NiUzxRBku2CODulBNSr7hEjI52P-UIfJ3YJm42la-oS1pq9jfNZ4VUmHtO2E3V3T2UnDVv5RGjYSFyCkpyf4wHw5TWJW7atAUev1Q1pugQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)] [[code](https://github.com/MrtnMndt/OpenVAE_ContinualLearning)]
- (**CoLLAs 2022**) Continual Learning with Foundation Models: An Empirical Study of Latent Replay [[paper](https://proceedings.mlr.press/v199/ostapenko22a.html)] [[code](https://github.com/oleksost/latent_CL)]


### 2021
- (**CVPR 2021**) Hyper-LifelongGAN: Scalable Lifelong Learning for Image Conditioned Generation [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Zhai_Hyper-LifelongGAN_Scalable_Lifelong_Learning_for_Image_Conditioned_Generation_CVPR_2021_paper.html)]
- (**CVPR 2021**) Efficient Feature Transformations for Discriminative and Generative Continual Learning [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Verma_Efficient_Feature_Transformations_for_Discriminative_and_Generative_Continual_Learning_CVPR_2021_paper.html)] [[code](https://github.com/vkverma01/EFT)]
- (**ICCV 2021**) Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Smith_Always_Be_Dreaming_A_New_Approach_for_Data-Free_Class-Incremental_Learning_ICCV_2021_paper.html)] [[code](https://github.com/GT-RIPL/AlwaysBeDreaming-DFCIL)]
- (**NeurIPS 2021**) CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks [[paper](https://proceedings.neurips.cc/paper/2021/hash/8073bd4ed0fe0c330290c58056a2cd5e-Abstract.html)] [[code](https://github.com/sakshivarshney/CAM-GAN)]
- (**NeurIPS 2021**) Generative vs. Discriminative: Rethinking The Meta-Continual Learning [[paper](https://papers.nips.cc/paper/2021/hash/b4e267d84075f66ebd967d95331fcc03-Abstract.html)] [[code](https://github.com/aminbana/GeMCL)]
- (**IJCNN 2021**) Generative Feature Replay with Orthogonal Weight Modification for Continual Learning [[paper](https://ieeexplore.ieee.org/abstract/document/9534437/)]

 
### 2020
- (**CVPR 2020**) Dreaming to distill: Data-free knowledge transfer via deepinversion [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html)] [[code](https://github.com/NVlabs/DeepInversion)]
- (**ECCV 22020**) Piggyback GAN: Efficient Lifelong Learning for Image Conditioned Generation [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660392.pdf)]
- (**NeurIPS 2020**) GAN memory with no forgetting [[paper](https://papers.nips.cc/paper/2020/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf)] [[code](https://github.com/MiaoyunZhao/GANmemory_LifelongLearning)]
- (**Nature Communications 2020**) Brain-inspired replay for continual learning with artificial neural networks [[paper](https://www.nature.com/articles/s41467-020-17866-2.pdf)] [[code](https://github.com/GMvandeVen/brain-inspired-replay)]
- (**Neurocomputing 2020**) Lifelong generative modeling [[paper](https://github.com/jramapuram/LifelongVAE)] [[code](https://www.sciencedirect.com/science/article/pii/S0925231220303623#bib0115)]
- (**IJCNN 2020**) Catastrophic forgetting and mode collapse in GANs [[paper](https://ieeexplore.ieee.org/abstract/document/9207181)] [[code](https://github.com/htt210/CatastrophicGANCode)]
- (**PhD Thesis 2020**) Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes [[paper](https://arxiv.org/pdf/2007.00487)]
- (**ICLR-W 2020**) Brain-like replay for continual learning with artificial neural networks [[paper](https://baicsworkshop.github.io/pdf/BAICS_8.pdf)]


### 2019
- (**CVPR 2019**) Learning to remember: A synaptic plasticity driven framework for continual learning [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html)] [[code](https://github.com/SAP-archive/machine-learning-dgm)]
- (**IJCAI 2019**) Closed-loop Memory GAN for Continual Learning [[paper](https://www.ijcai.org/proceedings/2019/0462.pdf)]
- (**ICCV 2019**) Lifelong GAN: Continual learning for conditional image generation [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html)]
- (**IJCAI 2019**) Complementary Learning for Overcoming Catastrophic Forgetting Using Experience Replay [[paper](https://www.ijcai.org/proceedings/2019/0463.pdf)]
- (**NeurIPS 2019**) Continual Unsupervised Representation Learning [[paper](https://proceedings.neurips.cc/paper/2019/hash/861578d797aeb0634f77aff3f488cca2-Abstract.html)] [[code](https://github.com/google-deepmind/deepmind-research/tree/master/curl)]
- (**IJCNN 2019**) Generative Models from the perspective of Continual Learning [[paper](https://ieeexplore.ieee.org/abstract/document/8851986/)] [[code](https://github.com/TLESORT/Generative_Continual_Learning)]
- (**ICANN 2019**) Marginal replay vs conditional replay for continual learning [[paper](https://arxiv.org/pdf/1810.12069)]


### 2018
- (**ICLR 2018**) Variational Continual Learning [[paper](https://openreview.net/pdf?id=BkQqq0gRb)] [[code](https://github.com/nvcuong/variational-continual-learning)]
- (**ICLR 2018**) Memorization precedes generation: Learning unsupervised GANs with memory networks [[paper](https://openreview.net/pdf?id=rkO3uTkAZ)] [[code](https://github.com/whyjay/memoryGAN)]
- (**NeurIPS 2018**) Memory Replay GANs: Learning to Generate New Categories without Forgetting [[paper](https://proceedings.neurips.cc/paper/2018/hash/a57e8915461b83adefb011530b711704-Abstract.html)] [[code](https://github.com/WuChenshen/MeRGAN)]
- (**BMVC 2018**) Exemplar-Supported Generative Reproduction for Class Incremental Learning [[paper](http://bmvc2018.org/contents/papers/0325.pdf)] [[code](https://github.com/TonyPod/ESGR)]
- (**NeurIPS-W 2018**) Improving and Understanding Variational Continual Learning [[paper](https://arxiv.org/pdf/1905.02099)] [[code](https://github.com/nvcuong/variational-continual-learning/tree/master/improved_ddm)]
- (**NeurIPS-W 2018**) Continual Classification Learning Using Generative Models [[paper](https://arodes.hes-so.ch/record/4159?ln=en&v=pdf)]
- (**NeurIPS-W 2018**) Self-Supervised GAN to Counter Forgetting [[paper](https://arxiv.org/pdf/1810.11598)]
- (**arXiv 2018**) Generative replay with feedback connections as a general strategy for continual learning [[paper](https://arxiv.org/abs/1809.10635)] [[code](https://github.com/GMvandeVen/continual-learning)]


### 2017
- (**NeurIPS 2017**) Continual Learning with Deep Generative Replay [[paper](https://proceedings.neurips.cc/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html)]
- (**arXiv 2017**) Continual Learning in Generative Adversarial Nets [[paper](https://arxiv.org/pdf/1705.08395)]


### Pre-2017
- (**Connection Science 1995**) Catastrophic Forgetting, Rehearsal and Pseudorehearsal [[paper](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=5ac423a83b4321b43249224fcc528bb70e086826)]

---









## :clap: Contribute [[chinese version](http://t.csdnimg.cn/S1rvo)]
**1. Fork the Repository:** Click on the `Fork` button in the top-right corner to create a copy of the repository in your GitHub account.

**2. Create a New Branch:** In your forked repository, create a new branch (e.g., "libo") by using the branch selector button near the top-left (usually labeled `master` or `main`).

**3. Make Your Changes:** Switch to your new branch using the same selector. Then, click the `Edit file` button at the top right and make your changes. Add entries in the following format:
  ```bash
  - (**journal/conference_name year**) paper_name [[paper](online_paper_link)] [[code](online_code_link)]
  ```

**4. Commit Changes:** Save your changes by clicking the `Commit changes` button in the upper-right corner. Enter a commit message (e.g., "add 1 cvpr'24 paper") and an extended description if necessary, then confirm your changes by clicking the `Commit changes` button again at the bottom right.

**5. Create a Pull Request:** Go back to your forked repository and click `Compare & pull request`. Alternatively, select your branch from the branch selector and click `Open pull request` from the `Contribute` drop-down menu. Fill out the title and description for your pull request, and click `Create pull request` to submit it.


<div align="right">
  <a href="#awesome-incremental--continual--lifelong-generative-learning">:top: Back to top</a>
</div>
<div align="center">
  <img src="https://visitor-badge.laobi.icu/badge?page_id=libo-huang.Awesome-Incremental-Generative-Learning&left_color=green&right_color=red&format=true" alt="xxx">
</div>
